[
  {
    "objectID": "creations/tao_of_tim.html",
    "href": "creations/tao_of_tim.html",
    "title": "Creating the Tao of Tim",
    "section": "",
    "text": "Overview\nThe Tim Ferriss Show is a podcast produced by Tim Ferriss, which aims to deconstruct world-class performers and share their tools and tactics along the way.\nThe goal of this project is to programmatically extract the show links from each episode and create a job that runs each morning to serve up a random link. By doing this, I hope to automate the exposure to different ideas from top-performers.\n\n\nFinding The Data\n\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(here)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(xml2)\nlibrary(here)\nlibrary(telegram.bot)\n\nShow links are available for each podcast guest here. Links to the individual podcast episodes are located within the tim-ferriss-podcast-list div tag.\nThe only problem is - a handful of episodes are visible at one time. To expose more episodes, I need to click on the load-more-podcasts button which runs some javascript to access a Wordpress API to load additional podcasts from the tim_podcasts.endpoint.\nTo work around this, I figured I would use some Python and the selenium library to programatically click the load-more-podcasts button until it was disabled.\nThe selenium approach worked - to a degree - but there is javascript within Tim’s website that renders multiple call-to-action pages. I could spend more time finding a way around the javascript, but as I was thinking about how to do that I stumbled across the sitemap.\nNow this is what I’m looking for!\nNo need to create a headless browser and deal with all this javascript, I can instead get right to the links I’m after.\n\n\nGathering The URLs\nSince the site map is in xml format, I first need to use the xml2 package to parse the URL for each web page. Thankfully, there is a consistent naming convention so I can use the tidyr::separate_wider_regex() function to identify the upload date and title for each page.\n\n\nCode\n# Scrape Site Map and Clean\n\nraw_xml &lt;- xml2::read_xml(\"https://tim.blog/post-sitemap2.xml\")\n\nsite_df &lt;- raw_xml |&gt; \n  xml2::xml_ns_strip() |&gt; \n  xml2::xml_find_all(\".//url\") |&gt; \n  xml2::xml_find_all(\".//loc\") |&gt; \n  xml2::xml_text() |&gt; \n  tibble::as_tibble_col(column_name = \"urls\") |&gt; \n  tidyr::separate_wider_regex(\n    urls,\n    patterns = c(\n      \"https://tim.blog/\",\n      year = \"[:digit:]{4}\",\n      \"/\",\n      month = \"[:digit:]{2}\",\n      \"/\",\n      day = \"[:digit:]{2}\",\n      \"/\",\n      article = \".*\",\n      \"/\"\n    ),\n    cols_remove = FALSE\n  ) |&gt; \n  dplyr::mutate(\n    upload_date = lubridate::ymd(paste0(year, month, day)),\n    .keep = \"unused\"\n  )\n\n\nAfter a quick review of the URLs, several patterns start to stand out. First, Tim posts transcripts of each podcast episode on his site. He also posts several different flavors of recap episodes, along with content from other projects he has created, such as Tools of Titans.\nIf I make a list of keywords from the patterns identified above, combined with a filter on the upload date to strip out any URL that occurred before the first podcast episode, I should be able to pare down my dataframe to just the podcast episode web pages.\nBeing the Tim Ferriss Show connoisseur that I am, I also know that he took a sabbatical in the middle of 2024. To fill the content gap, he published “new” episodes that combined two past podcast episodes. Since I only want the show links for each original podcast, I will need to filter out this chunk of time as well.\n\n\nCode\n# disregard non-pertinent urls after manual review of site_df\nblack_list &lt;- c(\"transcript\", \"transcipt\", \"in-case-you-missed\",\n                \"recap\", \"tools-of-titans\", \"cockpunch\", \"top-\",\n                \"insights-from-\")\n\npodcast_df &lt;- site_df |&gt; \n  # filtering to on or after the first podcast episode\n  dplyr::filter(upload_date &gt;= as.Date(\"2014-04-22\")) |&gt;\n  # removing a stretch of time where old podcasts were combined to make a new podcast\n  dplyr::filter(upload_date &gt; as.Date(\"2024-08-29\") |\n                  upload_date &lt; as.Date(\"2024-05-16\")) |&gt;\n  dplyr::filter(stringr::str_detect(article, paste(black_list, collapse = \"|\")) == FALSE) |&gt; \n  # removing one-off recap that would cause duplicate show links\n  dplyr::filter(article != \"the-30-most-popular-episodes-of-the-tim-ferriss-show-from-2022\")\n\n\nAnd with that, I have a dataframe of each Tim Ferriss Show podcast episode and its upload date! Now, it’s time to get to scraping.\n\n\nScraping The Episodes\nSince I am a fan of Tim’s, and certainly not trying to get in trouble with him (if you’re reading this Tim, hello!), I want to be respectful while I’m scraping. Enter, the polite package. By using polite::bow(), I can engage with the host once, gain an understanding for the robots.txt file that is in place, and obey the scraping limitations while gathering the data I’m looking for.\nBy setting up a little function, I can polite::nod() to each podcast URL to continue my single point of contact with the host while scraping under the prescribed parameters. Using the rvest package, I can gather both the text and the href attribute for each show note link. Bundling this function with the purrr::map() function, I can iterate over each URL and build how the final show links dataframe.\n\n\nCode\nsession &lt;- polite::bow(\"https://tim.blog/\")\n\nget_show_links &lt;- function(url) {\n  tryCatch(\n    {\n      # create throwaway list for each list item on a podcast web page\n      foo &lt;- session |&gt; \n        polite::nod(path = url) |&gt;\n        polite::scrape() |&gt; \n        rvest::html_elements(\".wp-block-list li a\")\n      \n      # build dataframe from throwaway list to capture link title and link URL\n      bar &lt;- data.frame(\n        link_title = foo |&gt; rvest::html_text(),\n        link_url = foo |&gt; rvest::html_attr(\"href\")\n      )\n      \n      return(bar)\n    }, \n    \n    error = function(msg) {\n      message(paste(\"The article\", url, \"encountered an issue when scraping show links.\"))\n      return(NA)\n    }\n  )\n}\n\n# need to unnest the show_link column which returns a dataframe for each podcast URL to tidy the data\nshow_links_df &lt;- podcast_df |&gt; \n  dplyr::mutate(show_links = purrr::map(urls, get_show_links)) |&gt; \n  tidyr::unnest_longer(show_links) |&gt; \n  tidyr::unnest_wider(show_links)\n\n\n\n\nSending The Show Links\nWith my dataframe of show note links in-hand, I just need to find a way to send myself a random link each morning. The random link piece is straight forward, I can use the dplyr::slice_sample() function to pull out a new link each day. The part that is more involved is setting up a way to get the link to me.\nEnter, GitHub Actions and Telegram. By using GitHub Actions, I can automatically run a script to randomly select a show link. Using Telegram, and the telegram.bot R package, I can send the show link as a Telegram message to either myself or a Telegram channel.\nQuite a bit has been written about setting up a Telegram bot. Instead of adding more, I will instead point you towards a helpful blog post written by my friend Brad Lindblad. In the post, he automates the scraping of weekly meat specials using Python and Telegram.\nAfter following along with Brad’s blog post, I now have a Telegram bot. I’ve tested the telegram.bot::bot() setup and have sent a few test messages to myself using bot$sendMessage(). I think I’m ready to combine the link selection and message sending into a script.\nLooking at the bot$sendMessage() documentation, I see there is a parse_mode argument. Since I have show links, and I want to provide the URL to the link that was selected, I need to use the markdown option to properly format my message. With a little magic from paste0(), I create the variable telegram_message which is assigned a string that matches the markdown syntax I need to send out the message.\nWhere this work differs from Brad’s post lies with the GitHub Action used to run the random link script. Since I am working in R and I am using the renv package to manage my project environment, I need to include a couple different jobs to get things working. The R community has a convenient repository of GitHub Actions available so I can just plug in both the setup-r@v2 and setup-renv@v2 GitHub Actions and I am off and running. You can see the full workflow below.\n\nname: Tao te Tim Telegram\n\non: \n  workflow_dispatch:\n  schedule:\n    -  cron: '30 6 * * *'\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      -  name: Checkout repository\n         uses: actions/checkout@v4\n\n      -  name: Install R\n         uses: r-lib/actions/setup-r@v2\n         with:\n           r-version: '4.4.3'\n\n      -  name: Install libcurl\n         if: runner.os == 'Linux'\n         run: sudo apt-get update -y && sudo apt-get install -y libcurl4-openssl-dev\n\n      -  name: Set up renv\n         uses: r-lib/actions/setup-renv@v2\n\n      -  name: Run send_telegram\n         run: Rscript -e 'source(\"R/send_telegram.R\")'\n         env:\n           R_TELEGRAM_BOT_TIM_TAO_BOT: ${{ secrets.R_TELEGRAM_BOT_TIM_TAO_BOT }}\n           TELEGRAM_CHANNEL_ID:      ${{ secrets.TELEGRAM_CHANNEL_ID }}\n\nOne word of caution as it relates to the secrets. I assumed a secret that was a string value needed to be entered into GitHub in the same fashion, using quotes. This is not the case. If you do, you will receive an error when running the action. Hopefully this will save you some time if you find yourself in a similar situation.\n\n\nWrap Up\nWith a little help from Tim’s site map, I was able to locate and clean show notes from over 700 podcast episodes. Combined with just a bit of set up to create a Telegram bot, along with a short GitHub Actions script, I now get a new show note link each morning. And if you’re feeling left out, don’t worry. You can join the Tao of Tim Telegram channel too!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "A Glimpse at Past Projects",
    "section": "",
    "text": "flashcrash A Shiny app that retrieves a random chart from the Animal Spirits podcast.\nTao of Tim Automating exposure to top-performers one Telegram message at a time."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "What I’m Up to Now",
    "section": "",
    "text": "Last Updated: July 14th, 2025\nAs suggested by Derek Sivers, here is a list of the things I’m currently doing or thinking about."
  },
  {
    "objectID": "now.html#tao-of-tim",
    "href": "now.html#tao-of-tim",
    "title": "What I’m Up to Now",
    "section": "Tao of Tim",
    "text": "Tao of Tim\nThe Tao of Tim is now a Telegram channel! Feel free to subscribe and get a daily show note from The Tim Ferriss Show. Check out this article for details on how I pieced the project together."
  },
  {
    "objectID": "now.html#learning-polars",
    "href": "now.html#learning-polars",
    "title": "What I’m Up to Now",
    "section": "Learning Polars",
    "text": "Learning Polars\nKeeping with my trend of writing more Python, I’ve recently started to dig into the Polars library. The ability to use the power of Python with a syntax that is very close to R has been a great way to leverage existing knowledge while learning something new. Plus, it’s hard to complain about the more user-friendly method names when compared to Pandas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hey There! I’m Dylan",
    "section": "",
    "text": "Learn about who I am and what I do\n\n\nABOUT\n\n\n\n\n\n\n\n\nCheck out what I’m up to currently\n\n\nNOW\n\n\n\n\n\n\n\n\nTake a peek at my past projects\n\n\nPROJECTS"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A Little About Me",
    "section": "",
    "text": "I’m a data professional working for Farm Credit Services of America. I live in Fargo, North Dakota with my wife, Lindsey, and our goldendoodle, Ellie. I enjoy coffee, taking our dog for walks, working out, and listening to podcasts.\nI attended the University of North Dakota and obtained a Bachelors of Accountancy and a Bachelors of Business Adminstration, specializing in Economics.\nI began my professional life in finance before finding a career in data. These days, I work primarily with open-source technologies. I prefer to program using the R programming language and use Quarto as my means of communication.\nI have extensive analytics experience, having developed an enterprise reporting framework from scratch, creating a Center of Excellence to expand the impact of analytics, and building a loan decision engine which handles over $3 billion in loan commitments annually."
  },
  {
    "objectID": "now.html#mlb-data",
    "href": "now.html#mlb-data",
    "title": "What I’m Up to Now",
    "section": "MLB Data",
    "text": "MLB Data\nMy wife and I recently watched The Clubhouse on Netflix and we were blown away by the effort that goes into the 162-game season. One of the questions that came to our minds when watching was, how has the percentage of players from different countries change over time? It’s a straightforward data question but will be fun to explore!"
  },
  {
    "objectID": "now.html#tinkering-with-marimo",
    "href": "now.html#tinkering-with-marimo",
    "title": "What I’m Up to Now",
    "section": "Tinkering with Marimo",
    "text": "Tinkering with Marimo\nContinuing with writing more Python, I’ve recently been tinkering with Marimo notebooks. The reproducible execution is interesting and the dataframe viewer seems fairly beneficial as well. I plan to use Marimo to explore the MLB data question above."
  }
]